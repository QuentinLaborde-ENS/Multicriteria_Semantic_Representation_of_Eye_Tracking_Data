import numpy as npimport pandas as pdimport matplotlib.pyplot as pltimport seaborn as snsimport plotly.figure_factory as ffimport pickle from pathlib import Pathfrom enum import Enumfrom typing import Dict, List, Optional, Union, Tuplefrom sklearn.cluster import KMeansfrom sklearn.decomposition import KernelPCAfrom scipy.spatial.distance import cdist, squareformfrom fastcluster import linkage   class Dataset(Enum):    """Enum for supported datasets."""     CLDRIVE = "CLDrive"    ETRA = "ETRA"    GAZEBASE = "GazeBase"class ClusteringMethod(Enum):    """Enum for supported clustering methods."""     KPCA_KMEANS = "kpca"class Symbolization:    """Class for symbolizing eye-tracking feature data across multiple datasets and clustering methods."""        def __init__(self, config: Dict, path: str, records: List[str], dataset: str = "CLDrive",                  clustering_method: str = "kpca_kmeans"):        """        Initialize Symbolization with configuration, path, records, dataset, and clustering method.        Args:            config: Configuration dictionary with symbolization parameters            path: Directory path containing feature data files            records: List of feature record filenames            dataset: Dataset identifier (e.g., "CLDrive", "ETRA", "GazeBase")            clustering_method: Clustering method ("kpca_kmeans")                Raises:            ValueError: If dataset or clustering method is not supported        """        self.config = config        self.path = Path(path)        self.records = records        try:            self.dataset = Dataset(dataset)        except ValueError as e:             raise ValueError(f"Unsupported dataset: {dataset}. Must be one of {', '.join(d.value for d in Dataset)}")                try:            self.clustering_method = ClusteringMethod(clustering_method)        except ValueError as e:             raise ValueError(f"Unsupported clustering method: {clustering_method}. Must be one of {', '.join(m.value for m in ClusteringMethod)}")                self.process_oculomotor = config.get('process_oculomotor', True)        self.process_scanpath = config.get('process_scanpath', True)        self.process_aoi = config.get('process_aoi', True)       def process(self) -> None:        """Process and symbolize features for each type."""                if self.process_oculomotor:            print("Symbolizing oculomotor fixation features...")            oculomotor_records = [r for r in self.records if r.split('.')[0].split('_')[-1] == 'oculomotor']            fix_features = [f for f in self.config['data']['oculomotor_features'] if f.startswith('fix')]            self._process_subset(oculomotor_records, fix_features, 'oculomotorFixation')            print("Symbolizing oculomotor saccade features...")            sac_features = [f for f in self.config['data']['oculomotor_features'] if f.startswith('sac')]            self._process_subset(oculomotor_records, sac_features, 'oculomotorSaccade')                if self.process_scanpath:            print("Symbolizing scanpath features...")            scanpath_records = [r for r in self.records if r.split('.')[0].split('_')[-1] == 'scanpath']            sp_features = [f for f in self.config['data']['scanpath_features'] if f.startswith('Sp')]            self._process_subset(scanpath_records, sp_features, 'scanpath')                if self.process_aoi:            print("Symbolizing AoI features...")            aoi_records = [r for r in self.records if r.split('.')[0].split('_')[-1] == 'aoi']            aoi_features = [f for f in self.config['data']['aoi_features'] if f.startswith('AoI')]            self._process_subset(aoi_records, aoi_features, 'AoI')    def _process_subset(self, feature_records: List[str], feature_set: List[str], type_: str,                         display: bool = False) -> None:        """Process a subset of features with the specified clustering method."""        n_centers = self._get_n_centers(type_)        bkpt_path = f'output/{self.dataset.value}/segmentation/'        outpath = f'output/{self.dataset.value}/symbolization_{self.clustering_method.value}/'        Path(outpath).mkdir(parents=True, exist_ok=True)         self._process_all(feature_records, feature_set, type_, n_centers, bkpt_path, outpath, display)            def _process_all(self, feature_records: List[str], feature_set: List[str], type_: str,                      n_centers: int, bkpt_path: str, outpath: str, display: bool) -> None:        """Process symbolization across all data."""        try:            sub_data, lengths = self._aggregate_data(feature_records, feature_set, bkpt_path, type_)            labels, centers, dist_mat = self._cluster_data(sub_data, n_centers)            ordered_dist_mat, res_order, ordered_labels, ordered_centers = self._reorder_clusters(labels, centers, dist_mat)            result_dict = self._build_result_dict(feature_records, feature_set, type_, bkpt_path, ordered_labels, ordered_centers, lengths)                         self._save_results(result_dict, f"{outpath}{type_}.pkl")             if display:                self._display_distance_matrix(ordered_dist_mat, type_)                first_key = next(iter(result_dict['recordings']))                self._plot_scarf(ordered_labels[0], result_dict['recordings'][first_key]['lengths'], type_)        except Exception as e:            print(f"Error: {e}")    def _aggregate_data(self, records: List[str], feature_set: List[str], bkpt_path: str, type_: str) -> Tuple[np.ndarray, List[List[int]]]:        """Aggregate data from records based on breakpoints."""        sub_data = []        lengths_per_record = []        for record in records:            if self._should_process_record(record):                try:                    df = pd.read_csv(self.path / record)[feature_set].to_numpy()                    bkpt_name = self._get_bkpt_name(record, type_)                    bkpts = np.load(Path(bkpt_path) / bkpt_name, allow_pickle=True)                    record_lengths = []                    for i in range(1, len(bkpts)):                        l_data = df[bkpts[i-1]:bkpts[i]]                        l_means = np.mean(l_data, axis=0)                        sub_data.append(l_means)                        record_lengths.append(bkpts[i] - bkpts[i-1])  # Corrected sign                    lengths_per_record.append(record_lengths)                except Exception as e:                    print(f"Error: {e}")        return np.array(sub_data), lengths_per_record    def _cluster_data(self, data: np.ndarray, n_centers: int) -> Tuple[np.ndarray, Optional[np.ndarray], np.ndarray]:        """Cluster data using the specified method."""        try:            if self.clustering_method == ClusteringMethod.KPCA_KMEANS:                kpca = KernelPCA(n_components=10, kernel="rbf", n_jobs=-1)                transformed_data = kpca.fit_transform(data)                kmeans = KMeans(n_clusters=n_centers, n_init=100, random_state=0).fit(transformed_data)                return kmeans.labels_, kmeans.cluster_centers_, cdist(kmeans.cluster_centers_, kmeans.cluster_centers_)        except Exception as e:             raise    def _reorder_clusters(self, labels: np.ndarray, centers: Optional[np.ndarray], dist_mat: np.ndarray) -> Tuple[np.ndarray, List[int], List[List[int]], np.ndarray]:        """Reorder clusters based on pairwise distances."""        try:            ordered_dist_mat, res_order, res_linkage = self.compute_serial_matrix(dist_mat, 'ward')            inv_res_order = np.zeros(len(res_order), dtype=int)            for k, v in enumerate(res_order):                inv_res_order[v] = k            re_ordering = lambda x: [int(inv_res_order[x[i]]) for i in range(len(x))]            ordered_labels = [re_ordering(labels)]            ordered_centers = centers[res_order] if centers is not None else np.array([])  # Efficient NumPy indexing            return ordered_dist_mat, res_order, ordered_labels, ordered_centers        except Exception as e:             raise    def _build_result_dict(self, records: List[str], feature_set: List[str], type_: str,                            bkpt_path: str, ordered_labels: List[List[int]], ordered_centers: np.ndarray,                            lengths_per_record: List[List[int]]) -> Dict:        """Build result dictionary with sequences and lengths."""        result_dict = {'centers': ordered_centers, 'recordings': {}}        label_idx = 0        for idx, record in enumerate(records):            if self._should_process_record(record):                name = self._get_record_name(record)                record_lengths = lengths_per_record[idx]                n_segments = len(record_lengths)                result_dict['recordings'][name] = {                    'sequence': ordered_labels[0][label_idx:label_idx + n_segments],                    'lengths': record_lengths                }                label_idx += n_segments                 return result_dict    def _should_process_record(self, record: str) -> bool:        """Check if a record should be processed based on dataset-specific conditions."""        parts = record.split('.')[0].split('_')        config_data = self.config['data']        try:             if self.dataset == Dataset.CLDRIVE:                return (len(parts) >= 2 and parts[1] in config_data.get('label_set', []))            elif self.dataset == Dataset.ETRA:                return (len(parts) >= 5 and                         parts[0] in config_data.get('subject_set', []) and                         parts[2] in config_data.get('task_set', []) and                         parts[3] in config_data.get('condition_set', []))            elif self.dataset == Dataset.GAZEBASE:                return (len(parts) >= 3 and                         parts[2] in config_data.get('label_set', []) and                         parts[1] in config_data.get('session', []))        except Exception as e:            print(f"Error: {e}")        return False    def _get_n_centers(self, type_: str) -> int:        """Get number of clusters based on feature type."""        clusters = self.config['symbolization']['nb_clusters']        return clusters.get(type_, clusters.get('oculomotor')) if type_ in ['scanpath', 'AoI'] else clusters.get('oculomotor', 3)    def _get_bkpt_name(self, record: str, type_: str) -> str:        """Generate breakpoint file name based on dataset."""        parts = record.split('.')[0].split('_')         if self.dataset == Dataset.CLDRIVE:            return f'{parts[0]}_{parts[1]}_{type_}.npy'        elif self.dataset == Dataset.ETRA:            return f'{parts[0]}_{parts[1]}_{parts[2]}_{parts[3]}_{parts[4]}_{type_}.npy'        elif self.dataset == Dataset.GAZEBASE:            return f'{parts[0]}_{parts[1]}_{parts[2]}_{type_}.npy'    def _get_record_name(self, record: str) -> str:        """Generate record name based on dataset."""        parts = record.split('.')[0].split('_')         if self.dataset == Dataset.CLDRIVE:            return '_'.join(parts[:2])        elif self.dataset == Dataset.ETRA:            return '_'.join(parts[:5])        elif self.dataset == Dataset.GAZEBASE:            return '_'.join(parts[:3])    @staticmethod    def _display_distance_matrix(dist_mat: np.ndarray, title: str) -> None:        """Display the ordered distance matrix."""        plt.style.use("seaborn-v0_8")        plt.imshow(dist_mat, cmap='viridis')        plt.colorbar(label='Distance')        plt.grid(None)        plt.title(title)        plt.show()        plt.clf()    @staticmethod    def _plot_scarf(labels: List[int], lengths: List[int], title: str) -> None:        """Generate and display a scarf plot."""        try:            s_lengths = np.cumsum([0] + lengths)            starts, ends = s_lengths[:-1], s_lengths[1:]            df = pd.DataFrame([dict(Task="0", Start=starts[i], Finish=ends[i], Resource=labels[i])                                for i in range(len(starts))])            colors_sns = sns.color_palette("viridis", n_colors=max(labels) + 1)            colors = {idx: colors_sns[idx] for idx in sorted(set(labels))}            fig = ff.create_gantt(df, index_col="Resource", bar_width=0.4, show_colorbar=True,                                   group_tasks=True, colors=colors)            fig.update_layout(xaxis_type="linear", height=400, width=max(300, len(starts) * 80),                              xaxis_title="Time (s)", yaxis_title=f"{title} Sequence Index",                              legend=dict(title=dict(text="Clusters")))            fig.show()        except Exception as e:            print(f"Error: {e}")    @staticmethod    def _save_results(result_dict: Dict, filename: str) -> None:        """Save results to a pickle file."""        try:            with open(filename, 'wb') as fp:                pickle.dump(result_dict, fp)        except Exception as e:            print(f"Error: {e}")    @staticmethod    def compute_serial_matrix(dist_mat: np.ndarray, method: str = "ward") -> Tuple[np.ndarray, List[int], np.ndarray]:        """Compute serial matrix for cluster reordering."""        N = len(dist_mat)        flat_dist_mat = squareform(dist_mat)        res_linkage = linkage(flat_dist_mat, method=method, preserve_input=True)        res_order = Symbolization.seriation(res_linkage, N, N + N - 2)        seriated_dist = np.zeros((N, N))        a, b = np.triu_indices(N, k=1)        seriated_dist[a, b] = dist_mat[[res_order[i] for i in a], [res_order[j] for j in b]]        seriated_dist[b, a] = seriated_dist[a, b]        return seriated_dist, res_order, res_linkage    @staticmethod    def seriation(Z: np.ndarray, N: int, cur_index: int) -> List[int]:        """Compute the order implied by a hierarchical tree (dendrogram)."""        if cur_index < N:            return [cur_index]        left = int(Z[cur_index - N, 0])        right = int(Z[cur_index - N, 1])        return Symbolization.seriation(Z, N, left) + Symbolization.seriation(Z, N, right)def process(config: Dict, path: str, records: List[str], dataset: str = "CLDrive",             clustering_method: str = "kmeans") -> None:    """Functional wrapper for symbolization."""    try:        symbolization = Symbolization(config, path, records, dataset, clustering_method)        symbolization.process()    except Exception as e:        raise